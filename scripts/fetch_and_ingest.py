# -*- coding: utf-8 -*-
"""fetch_and_ingest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IkrqwUpaH_3W88cMBpTsb2eb8zSq5Za
"""

#!/usr/bin/env python3
"""
fetch_and_ingest.py

Downloads CSVs from a list of endpoints (configured in config/endpoints.json or
passed via --config) and saves them into data/incoming/. Optionally invokes the
ingest pipeline (scripts/ingest_and_prepare.py) after successful downloads.

Features:
- Supports HTTP(S) URLs and local file:// paths for testing
- Supports per-endpoint headers and simple authentication
- Deduplicates downloads by filename + checksum (skips identical files)
- Retry and backoff logic for robustness
- Writes simple logs to logs/fetch_and_ingest.log
- Can be scheduled (cron) or run manually

Usage:
    # Default: read config/endpoints.json and download + ingest
    python scripts/fetch_and_ingest.py --run

    # Provide a custom config file
    python scripts/fetch_and_ingest.py --run --config config/endpoints.json

    # Download only (do not run ingest)
    python scripts/fetch_and_ingest.py --run --no-ingest

Config format (endpoints.json) examples:

Simple list of URLs:
{
  "endpoints": [
    "https://data.gov.bh/labour/export?format=csv",
    "https://client.example.com/api/students?format=csv",
    "file:///mnt/data/Untitled25.ipynb"   # local test file (example)
  ]
}

Advanced list with per-entry headers:
{
  "endpoints": [
    {"url":"https://api.example.com/export.csv", "headers": {"Authorization":"Bearer X"}},
    {"url":"https://other.example.com/download", "headers": {"X-API-KEY":"abc"}}
  ]
}

Note about testing: you can include a file:// URL to test the full pipeline
locally. Example file path you uploaded earlier (for local testing):

    file:///mnt/data/Untitled25.ipynb

(This is only for local testing - the pipeline expects CSV files in production.)
"""

import argparse
import hashlib
import json
import os
import shutil
import subprocess
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import requests

# -------------------- CONFIG & PATHS --------------------
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
INCOMING_DIR = os.path.join(PROJECT_ROOT, "data", "incoming")
LOG_DIR = os.path.join(PROJECT_ROOT, "logs")
CONFIG_DEFAULT = os.path.join(PROJECT_ROOT, "config", "endpoints.json")
INGEST_SCRIPT = os.path.join(PROJECT_ROOT, "scripts", "ingest_and_prepare.py")

os.makedirs(INCOMING_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

LOGFILE = os.path.join(LOG_DIR, "fetch_and_ingest.log")

# Download control
MAX_RETRIES = 3
RETRY_BACKOFF = 5  # seconds
CHUNK_SIZE = 8192

# -------------------- UTILITIES --------------------

def log(msg: str):
    ts = datetime.utcnow().isoformat()
    line = f"[{ts}] {msg}"
    print(line)
    try:
        with open(LOGFILE, "a", encoding="utf-8") as fh:
            fh.write(line + "\n")
    except Exception:
        pass


def safe_filename_from_url(url: str) -> str:
    # derive a stable, readable filename for incoming storage
    from urllib.parse import urlparse, unquote
    p = urlparse(url)
    name = os.path.basename(unquote(p.path)) or "download"
    host = p.hostname or "local"
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%S")
    # include short hash of url so different query strings still create unique name
    h = hashlib.md5(url.encode("utf-8")).hexdigest()[:8]
    fname = f"{host}_{ts}_{h}_{name}"
    # sanitize
    fname = fname.replace("..", "").replace("/", "_")
    return fname


def md5_of_file(path: str, block_size: int = 65536) -> str:
    h = hashlib.md5()
    with open(path, "rb") as fh:
        for block in iter(lambda: fh.read(block_size), b""):
            h.update(block)
    return h.hexdigest()


# -------------------- DOWNLOAD LOGIC --------------------

def _is_csv_like_from_headers(headers: dict) -> bool:
    ctype = headers.get("content-type", "").lower()
    for allowed in ALLOWED_CONTENT_TYPES:
        if allowed in ctype:
            return True
    # some APIs return text/csv; some set no content-type — fallback to heuristic later
    return False

def download_url_to_incoming(url: str, headers: Optional[Dict[str, str]] = None) -> Tuple[Optional[str], dict]:
    """
    Robust downloader:
      - supports file:// local files (copy)
      - returns (saved_path_or_None, status_dict)
      - status_dict contains keys: status in {'downloaded','duplicate','skipped','failed','warning'},
        and 'message' for human-readable info.
    """
    headers = headers or {}
    status = {"status": "failed", "message": "", "url": url}
    # handle local file copy
    if url.startswith("file://"):
        try:
            local_path = url[7:]
            if not os.path.exists(local_path):
                status["message"] = f"Local file not found: {local_path}"
                return None, status
            dest_name = safe_filename_from_url(local_path)
            dest_path = os.path.join(INCOMING_DIR, dest_name)
            shutil.copy2(local_path, dest_path)
            status.update({"status": "downloaded", "path": dest_path})
            log(f"Copied local file {local_path} -> {dest_path}")
            return dest_path, status
        except Exception as e:
            status["message"] = str(e)
            return None, status

    last_exc = None
    backoff = RETRY_BACKOFF
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = requests.get(url, headers=headers, stream=True, timeout=30)
            # do not raise here — we will handle non-200 gracefully
            if resp.status_code != 200:
                status["message"] = f"HTTP {resp.status_code}"
                log(f"Download failed {url} with HTTP {resp.status_code}")
                last_exc = Exception(status["message"])
                if attempt < MAX_RETRIES:
                    time.sleep(backoff); backoff *= 2; continue
                else:
                    return None, status

            # quick check header
            headers_resp = dict(resp.headers)
            csv_by_header = _is_csv_like_from_headers(headers_resp)

            # stream and cap size
            tmp_path = os.path.join(INCOMING_DIR, f".tmp_download_{int(time.time())}")
            total = 0
            with open(tmp_path, "wb") as fh:
                for chunk in resp.iter_content(chunk_size=CHUNK_SIZE):
                    if chunk:
                        fh.write(chunk)
                        total += len(chunk)
                        if total > MAX_DOWNLOAD_BYTES:
                            fh.close()
                            os.remove(tmp_path)
                            status["message"] = "file_too_large"
                            log(f"Aborting {url}: exceeds MAX_DOWNLOAD_BYTES")
                            return None, status

            # basic content validation
            with open(tmp_path, "rb") as fh:
                sample = fh.read(8192)
            # heuristic check if probably CSV
            csv_like = csv_by_header or is_likely_csv_bytes(sample)
            if not csv_like:
                # save to failed for manual review
                failed_name = safe_filename_from_url(url)
                failed_path = os.path.join(FAILED_DIR, failed_name + ".maybe_not_csv")
                shutil.move(tmp_path, failed_path)
                status.update({"status": "skipped", "message": "not_csv", "path": failed_path})
                log(f"Downloaded content not CSV-like: saved to {failed_path}")
                return failed_path, status

            # finally save with friendly name
            from urllib.parse import urlparse, unquote
            fname = os.path.basename(unquote(urlparse(url).path)) or "download.csv"
            dest_name = f"{hashlib.md5(url.encode('utf-8')).hexdigest()[:8]}_{int(time.time())}_{fname}"
            dest_path = os.path.join(INCOMING_DIR, dest_name)
            shutil.move(tmp_path, dest_path)

            # dedup by md5
            try:
                cur_md5 = md5_of_file(dest_path)
                for other in os.listdir(INCOMING_DIR):
                    if other == os.path.basename(dest_path):
                        continue
                    op = os.path.join(INCOMING_DIR, other)
                    if not os.path.isfile(op): 
                        continue
                    try:
                        if md5_of_file(op) == cur_md5:
                            os.remove(dest_path)
                            status.update({"status": "duplicate", "message": f"duplicate_of:{op}", "kept": op})
                            log(f"Duplicate found for {url}; kept existing {op}")
                            return op, status
                    except Exception:
                        continue
            except Exception:
                pass

            status.update({"status": "downloaded", "path": dest_path})
            log(f"Successfully downloaded {url} -> {dest_path}")
            return dest_path, status

        except Exception as e:
            last_exc = e
            status["message"] = str(e)
            log(f"Attempt {attempt} failed for {url}: {e}")
            if attempt < MAX_RETRIES:
                time.sleep(backoff)
                backoff *= 2
                continue
            else:
                return None, status
    return None, {"status": "failed", "message": str(last_exc), "url": url}



# -------------------- ENDPOINT HANDLING --------------------

def normalize_endpoint(entry) -> Tuple[str, Dict[str, str]]:
    """Accept either a string URL or an object {url, headers}. Return (url, headers)."""
    if isinstance(entry, str):
        return entry, {}
    if isinstance(entry, dict):
        return entry.get("url"), entry.get("headers", {})
    raise ValueError("Invalid endpoint entry: must be string or object")


def extract_urls_from_json_payload(payload) -> List[str]:
    urls = []

    def walk(obj):
        if isinstance(obj, dict):
            for v in obj.values():
                walk(v)
        elif isinstance(obj, list):
            for item in obj:
                walk(item)
        elif isinstance(obj, str):
            if obj.lower().endswith(".csv") or obj.startswith("http") or obj.startswith("file://"):
                urls.append(obj)

    walk(payload)
    return urls


# -------------------- MAIN WORKFLOW --------------------

def run_endpoints(endpoints: List, no_ingest: bool = False, dry: bool = False) -> List[Dict]:
    """
    Robust runner for endpoints:
      - handles string endpoints and dict {url, headers, ...}
      - uses download_url_to_incoming to get (path, status)
      - never raises for a single endpoint failure; logs results and continues
      - triggers ingest only if at least one file was successfully downloaded
    Returns a list of result dicts describing each endpoint outcome.
    """
    results = []
    for entry in endpoints:
        try:
            url, headers = normalize_endpoint(entry)
            if not url:
                log(f"Skipping invalid endpoint entry: {entry}")
                results.append({"endpoint": entry, "status": "invalid_entry"})
                continue

            log(f"Fetching: {url}")
            # download_url_to_incoming must return (path, status_dict)
            path, status = download_url_to_incoming(url, headers=headers)

            rec = {"endpoint": url, "status": status.get("status"), "message": status.get("message")}
            if status.get("status") == "downloaded" and path:
                rec["path"] = path
                results.append(rec)
                log(f"Downloaded and saved: {path}")
            elif status.get("status") in ("duplicate", "skipped"):
                # duplicate or skipped: include path/info for auditing
                rec.update({"path": status.get("path"), "note": status.get("message")})
                results.append(rec)
                log(f"Endpoint result (not new): {rec}")
            else:
                # failed or unknown: record error and continue
                rec["error"] = status.get("message")
                results.append(rec)
                log(f"Endpoint failed or skipped: {rec}")
                continue

        except Exception as e:
            # Defensive: never let a single unexpected error ruin the loop
            log(f"Unexpected error handling endpoint {entry}: {e}")
            results.append({"endpoint": str(entry), "status": "error", "error": str(e)})
            continue

    # Decide whether to run ingestion
    if not no_ingest:
        any_downloaded = any(r.get("status") == "downloaded" for r in results)
        if any_downloaded:
            if dry:
                log("Dry-run: downloads found but skipping actual ingest run")
            else:
                try:
                    run_ingest_script()
                except Exception as e:
                    log(f"Ingest script failed after downloads: {e}")
        else:
            log("No suitable downloads found; skipping ingest run")

    return results



def run_ingest_script():
    log(f"Running ingest script: {sys.executable} {INGEST_SCRIPT}")
    p = subprocess.run([sys.executable, INGEST_SCRIPT, "--run"], capture_output=True, text=True)
    log(f"Ingest stdout: {p.stdout.strip()[:1000]}")
    if p.stderr:
        log(f"Ingest stderr: {p.stderr.strip()[:1000]}")
    if p.returncode != 0:
        raise RuntimeError(f"Ingest script returned code {p.returncode}")


# -------------------- CONFIG LOADING --------------------

def load_config(path: str) -> List:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as fh:
        cfg = json.load(fh)
    endpoints = cfg.get("endpoints") or cfg.get("urls") or []
    return endpoints


# -------------------- CLI --------------------

def parse_args():
    p = argparse.ArgumentParser(description="Fetch CSVs from endpoints and optionally run ingest.")
    p.add_argument("--run", action="store_true", help="Execute fetch (and optional ingest)")
    p.add_argument("--config", type=str, default=CONFIG_DEFAULT, help="Path to endpoints JSON config")
    p.add_argument("--no-ingest", action="store_true", help="Do not run ingest script after downloads")
    p.add_argument("--dry", action="store_true", help="Dry-run mode: download only (or simulate) and do not run ingest")
    return p.parse_args()


def main():
    args = parse_args()
    if not args.run:
        print("No action specified. Use --run to fetch endpoints. Example:\n  python scripts/fetch_and_ingest.py --run")
        return
    try:
        endpoints = load_config(args.config)
    except Exception as e:
        log(f"Failed to load config {args.config}: {e}")
        return

    log(f"Starting fetch_and_ingest (endpoints: {len(endpoints)})")
    results = run_endpoints(endpoints, no_ingest=args.no_ingest, dry=args.dry)
    log("Fetch summary: " + json.dumps(results, ensure_ascii=False))
    print(json.dumps(results, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()