# -*- coding: utf-8 -*-
"""ingest_and_prepare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IkrqwUpaH_3W88cMBpTsb2eb8zSq5Za
"""

#!/usr/bin/env python3
"""
ingest_and_prepare.py

Automatically load incoming CSVs from `data/incoming/`, clean them, map them to
canonical master schemas, and merge/update master CSV files in
`data/bahrain_master/`.

Usage:
    # Dry run: show what would be done (no writes)
    python scripts/ingest_and_prepare.py --run --dry

    # Real run: process and update masters
    python scripts/ingest_and_prepare.py --run

    # Watch mode (auto-process new files as they arrive) - optional
    python scripts/ingest_and_prepare.py --watch

Notes:
- Place raw CSV files into data/incoming/ for processing.
- Backups of previous masters are saved under data/bahrain_master_backups/.
- The script is conservative: run with --dry first for new suppliers.

For quick local testing you can copy a local file into incoming or use a
file:// URL with fetch_and_ingest.py. Example uploaded test file path (for
developer/testing):
    /mnt/data/Untitled25.ipynb

(This file path is included only as an example test source - the pipeline
expects CSV files.)

"""

import argparse
import csv
import json
import os
import re
import shutil
import sys
import time
from copy import deepcopy
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import pandas as pd

# Optional: watchdog for watch mode
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except Exception:
    WATCHDOG_AVAILABLE = False

# -------------------- CONFIG --------------------
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
INCOMING_DIR = os.path.join(PROJECT_ROOT, "data", "incoming")
PROCESSED_DIR = os.path.join(INCOMING_DIR, "processed")
MASTER_DIR = os.path.join(PROJECT_ROOT, "data", "bahrain_master")
BACKUP_DIR = os.path.join(PROJECT_ROOT, "data", "bahrain_master_backups")

os.makedirs(INCOMING_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)
os.makedirs(MASTER_DIR, exist_ok=True)
os.makedirs(BACKUP_DIR, exist_ok=True)

# Expected master files and canonical schema (adjust to your real schema)
MASTER_FILES = {
    "labour_master.csv": {
        "required": ["year", "nationality", "sex", "total_workers"],
        "schema": ["year", "nationality", "sex", "total_workers"],
    },
    "occupation_workers.csv": {
        "required": ["year", "main_occupation", "nationality", "occupation_workers"],
        "schema": ["year", "main_occupation", "nationality", "occupation_workers"],
    },
    "households.csv": {
        "required": ["year", "governorate", "nationality", "households"],
        "schema": ["year", "governorate", "nationality", "households"],
    },
    "population_density.csv": {
        "required": ["year", "governorate", "population", "density"],
        "schema": ["year", "governorate", "population", "density"],
    },
    "housing_units.csv": {
        "required": ["year", "governorate", "housing_type", "units"],
        "schema": ["year", "governorate", "housing_type", "units"],
    },
    "students.csv": {
        "required": ["year", "sector", "level", "students"],
        "schema": ["year", "sector", "level", "students"],
    },
    "teachers.csv": {
        "required": ["year", "school_type", "level", "teachers"],
        "schema": ["year", "school_type", "level", "teachers"],
    },
    "higher_education.csv": {
        "required": ["year", "sector", "academic_programme", "students"],
        "schema": ["year", "sector", "academic_programme", "students"],
    },
}

# Column hint patterns (regex -> canonical column)
COLUMN_HINTS = {
    r"^year$": "year",
    r"^yr$": "year",
    r"^academic_year$": "year",
    r"governorate|gov_name|region": "governorate",
    r"nationalit(y|ies)|nat$|nat_name": "nationality",
    r"sex|gender": "sex",
    r"total_?workers|workers_total|number_of_workers|no_of_workers|workers": "total_workers",
    r"occupation|main_occupation|job_title|occupation_name": "main_occupation",
    r"occupation_?workers|occupation_workers|workers_in_occupation": "occupation_workers",
    r"household|households|no_of_households|number_of_households": "households",
    r"population(?!_density)|pop_total": "population",
    r"density|population_density": "density",
    r"units|no_of_units|number_of_units|housing_unit|housing_units": "units",
    r"housing_type|unit_type|type_of_unit": "housing_type",
    r"student|students|no_of_students|number_of_students": "students",
    r"teacher|teachers|no_of_teachers|number_of_teachers": "teachers",
    r"school_type|sector": "sector",
    r"academic_programme|programme|program|major": "academic_programme",
    r"level|education_level|grade": "level",
}

FILENAME_KEYWORDS = {
    "labour": "labour_master.csv",
    "workers": "labour_master.csv",
    "occupation": "occupation_workers.csv",
    "household": "households.csv",
    "population": "population_density.csv",
    "density": "population_density.csv",
    "housing": "housing_units.csv",
    "student": "students.csv",
    "teacher": "teachers.csv",
    "higher": "higher_education.csv",
}

# -------------------- HELPERS --------------------

def normalize_col_name(c: str) -> str:
    if c is None:
        return c
    c2 = re.sub(r"\s+", "_", c.strip().lower())
    c2 = re.sub(r"[^\w_]", "", c2)
    return c2


def smart_read_csv(path: str) -> pd.DataFrame:
    # try common encodings
    for enc in (None, "utf-8-sig", "latin-1"):
        try:
            if enc:
                return pd.read_csv(path, encoding=enc)
            else:
                return pd.read_csv(path)
        except Exception:
            continue
    # last resort: read with python engine
    try:
        return pd.read_csv(path, engine="python", encoding="latin-1")
    except Exception:
        return pd.DataFrame()


def coerce_numeric_series(s: pd.Series) -> pd.Series:
    if s.dtype == object:
        s = s.str.replace(",", "", regex=False)
        s = s.str.replace(" ", "", regex=False)
    return pd.to_numeric(s, errors="coerce")


def map_columns_by_hints(cols: List[str]) -> Dict[str, str]:
    mapping = {}
    for col in cols:
        nc = normalize_col_name(col)
        for pat, can in COLUMN_HINTS.items():
            if re.search(pat, nc):
                mapping[col] = can
                break
    return mapping


def detect_target_by_filename(fname: str) -> Optional[str]:
    n = fname.lower()
    for k, t in FILENAME_KEYWORDS.items():
        if k in n:
            return t
    return None


def detect_target_by_columns(cols: List[str]) -> Optional[str]:
    best = None
    best_score = 0
    norm_cols = [normalize_col_name(c) for c in cols]
    for master_fname, meta in MASTER_FILES.items():
        score = 0
        for req in meta.get("required", []):
            if req in norm_cols:
                score += 2
            else:
                for pat, can in COLUMN_HINTS.items():
                    if can == req:
                        # if any hint pattern matches any incoming column, add 1
                        for nc in norm_cols:
                            if re.search(pat, nc):
                                score += 1
                                break
        if score > best_score:
            best_score = score
            best = master_fname
    return best if best_score >= 2 else None


def standardize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()
    df = df.copy()
    df.columns = [c.strip() for c in df.columns]
    # create mapping of original -> normalized
    norm_map = {c: normalize_col_name(c) for c in df.columns}
    df.rename(columns=norm_map, inplace=True)
    for c in df.select_dtypes(include="object").columns:
        df[c] = df[c].astype(str).str.strip()
    return df


def map_to_master_schema(df: pd.DataFrame, target_master: str) -> Tuple[pd.DataFrame, Dict[str, str]]:
    if df.empty:
        return df, {}
    incoming_cols = list(df.columns)
    mapping = {}
    # direct matches
    norm_to_incoming = {normalize_col_name(c): c for c in incoming_cols}
    for can in MASTER_FILES[target_master]["schema"]:
        if can in norm_to_incoming:
            mapping[norm_to_incoming[can]] = can
    # hints
    hints = map_columns_by_hints(incoming_cols)
    for inc, can in hints.items():
        if inc not in mapping and can in MASTER_FILES[target_master]["schema"]:
            mapping[inc] = can
    # fallback fuzzy substring
    for inc in incoming_cols:
        if inc in mapping:
            continue
        nin = normalize_col_name(inc)
        for can in MASTER_FILES[target_master]["schema"]:
            if can in nin or nin in can:
                mapping[inc] = can
                break
    # build output
    out = pd.DataFrame()
    for inc, can in mapping.items():
        out[can] = df[inc]
    for can in MASTER_FILES[target_master]["schema"]:
        if can not in out.columns:
            out[can] = pd.NA
    # normalize types
    if "year" in out.columns:
        def extract_year_value(x):
            if pd.isna(x):
                return pd.NA
            s = str(x)
            m = re.search(r"(19[0-9]{2}|20[0-9]{2}|2100)", s)
            if m:
                return int(m.group(0))
            try:
                return int(float(s))
            except Exception:
                return pd.NA
        out["year"] = out["year"].apply(extract_year_value)
    for c in out.columns:
        if re.search(r"(_?workers$|workers$|units$|population$|households$|students$|teachers$|_count$|_num$|density$)", c):
            out[c] = coerce_numeric_series(out[c])
    for c in out.select_dtypes(include="object").columns:
        out[c] = out[c].str.strip()
    return out, mapping


def merge_into_master(mapped_df: pd.DataFrame, master_filename: str, dry: bool = False) -> Dict[str, int]:
    out_path = os.path.join(MASTER_DIR, master_filename)
    stats = {"master_exists": False, "rows_incoming": len(mapped_df), "rows_before": 0, "rows_after": 0}
    if not os.path.exists(out_path):
        stats["master_exists"] = False
        if not dry:
            mapped_df.to_csv(out_path, index=False)
            stats["rows_after"] = len(mapped_df)
        else:
            stats["rows_after"] = len(mapped_df)
        return stats
    master_df = smart_read_csv(out_path)
    master_df = standardize_dataframe(master_df)
    stats["master_exists"] = True
    stats["rows_before"] = len(master_df)
    # ensure columns
    for col in mapped_df.columns:
        if col not in master_df.columns:
            master_df[col] = pd.NA
    merged = pd.concat([master_df, mapped_df], ignore_index=True, sort=False)
    # dedupe
    before = len(merged)
    merged = merged.drop_duplicates()
    # build key cols
    key_cols = ["year"]
    for c in merged.columns:
        if c == "year":
            continue
        if merged[c].dtype == "object" or merged[c].dtype.name.startswith("category"):
            sample = merged[c].dropna().astype(str)
            if len(sample) > 0 and sample.map(len).mean() < 60:
                key_cols.append(c)
    if len(key_cols) <= 1:
        key_cols = [c for c in merged.columns]
    merged = merged.drop_duplicates(subset=key_cols, keep="last")
    after = len(merged)
    stats["rows_after"] = after
    if not dry:
        ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        try:
            shutil.copy2(out_path, os.path.join(BACKUP_DIR, f"{os.path.basename(out_path)}.{ts}.bak"))
        except Exception:
            pass
        schema = MASTER_FILES.get(master_filename, {}).get("schema", list(merged.columns))
        cols_to_write = [c for c in schema if c in merged.columns] + [c for c in merged.columns if c not in schema]
        merged.to_csv(out_path, index=False, columns=cols_to_write)
    return stats


def process_file(path: str, dry: bool = False) -> Dict:
    fname = os.path.basename(path)
    info = {"file": fname, "detected_master": None, "mapping": None, "merge_stats": None, "error": None}
    try:
        df = smart_read_csv(path)
        if df.empty:
            info["error"] = "Empty or unreadable CSV"
            return info
        df_std = standardize_dataframe(df)
        detected = detect_target_by_filename(fname)
        if not detected:
            detected = detect_target_by_columns(list(df_std.columns))
        if not detected:
            cols_join = " ".join(list(df_std.columns)).lower()
            if "worker" in cols_join or "employee" in cols_join:
                detected = "labour_master.csv"
            else:
                detected = list(MASTER_FILES.keys())[0]
        info["detected_master"] = detected
        mapped_df, mapping = map_to_master_schema(df_std, detected)
        info["mapping"] = mapping
        if "year" in mapped_df.columns:
            mapped_df = mapped_df.dropna(subset=["year"])
        for c in mapped_df.select_dtypes(include="object").columns:
            mapped_df[c] = mapped_df[c].str.strip()
        merge_stats = merge_into_master(mapped_df, detected, dry=dry)
        info["merge_stats"] = merge_stats
        # move processed file to processed/
        dst = os.path.join(PROCESSED_DIR, f"{fname}.processed_{int(time.time())}")
        try:
            shutil.copy2(path, dst)
        except Exception:
            pass
    except Exception as e:
        info["error"] = str(e)
    return info


def run_once(dry: bool = True, verbose: bool = True) -> List[Dict]:
    results = []
    for name in os.listdir(INCOMING_DIR):
        if not name.lower().endswith(".csv"):
            continue
        path = os.path.join(INCOMING_DIR, name)
        if os.path.isdir(path):
            continue
        if verbose:
            print(f"Processing: {path}")
        res = process_file(path, dry=dry)
        results.append(res)
        if verbose:
            print(json.dumps(res, indent=2, ensure_ascii=False))
    return results


class NewCSVHandler(FileSystemEventHandler):
    def __init__(self, dry: bool = False):
        super().__init__()
        self.dry = dry

    def on_created(self, event):
        if event.is_directory:
            return
        path = event.src_path
        if path.lower().endswith(".csv"):
            print(f"Detected new CSV: {path} - processing...")
            res = process_file(path, dry=self.dry)
            print(json.dumps(res, indent=2, ensure_ascii=False))


def watch_loop(dry: bool = False):
    if not WATCHDOG_AVAILABLE:
        print("[watch] watchdog package not installed. Install via `pip install watchdog`")
        return
    event_handler = NewCSVHandler(dry=dry)
    observer = Observer()
    observer.schedule(event_handler, INCOMING_DIR, recursive=False)
    observer.start()
    print(f"[watch] Watching directory: {INCOMING_DIR}")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()


def parse_args():
    p = argparse.ArgumentParser(description="Ingest incoming CSVs and prepare master files.")
    p.add_argument("--run", action="store_true", help="Process all files once (non-watch).")
    p.add_argument("--dry", action="store_true", help="Dry run: do not write master files; show actions.")
    p.add_argument("--watch", action="store_true", help="Watch incoming dir and process new CSVs automatically.")
    return p.parse_args()


def main():
    args = parse_args()
    if args.run:
        dry = args.dry
        print(f"Running ingest (dry={dry}) - incoming dir: {INCOMING_DIR}")
        res = run_once(dry=dry, verbose=True)
        print("Done. Summary:")
        print(json.dumps(res, indent=2, ensure_ascii=False))
    elif args.watch:
        dry = False
        if not WATCHDOG_AVAILABLE:
            print("Watch mode requested but watchdog is not available. Install: pip install watchdog")
            return
        watch_loop(dry=dry)
    else:
        print("No action specified. Use --run or --watch. Example:")
        print("  python scripts/ingest_and_prepare.py --run")
        print("  python scripts/ingest_and_prepare.py --run --dry")

if __name__ == "__main__":
    main()